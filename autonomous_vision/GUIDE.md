# ğŸ“˜ Guide Complet â€” SystÃ¨me de DÃ©tection d'Objets pour la Conduite Autonome

> **Projet PFE** | FÃ©vrier 2026  
> Ce guide vous accompagne de la prÃ©paration des donnÃ©es BDD100K jusqu'Ã  l'entraÃ®nement YOLO.

---

## ğŸ“… Calendrier RÃ©sumÃ©

| Jour | Ã‰tape | Objectif |
|------|-------|----------|
| 1 | Structure du projet | âœ… **Fait** â€” RÃ©pertoires et scripts crÃ©Ã©s |
| 1â€“2 | TÃ©lÃ©charger BDD100K | Obtenir images + labels depuis Berkeley |
| 2 | Convertir en YOLO | Transformer les labels JSON â†’ format YOLO |
| 3 | Filtrer les classes | Valider les 11 classes cibles |
| 4 | Nettoyer le dataset | Supprimer les fichiers invalides |
| 5 | Ã‰quilibrer le dataset | Corriger les classes sous-reprÃ©sentÃ©es |
| 6 | Diviser le dataset | Train 70% / Val 20% / Test 10% |
| 7+ | EntraÃ®ner YOLO | Lancer l'entraÃ®nement sur Kaggle |

---

## ğŸ§© Ã‰TAPE 1 â€” Structure du Projet âœ… FAIT

```
autonomous_vision/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ bdd100k/          â† BDD100K extrait (images + labels JSON)
â”‚   â”œâ”€â”€ raw/              â† Converti en format YOLO
â”‚   â”œâ”€â”€ processed/        â† AprÃ¨s filtrage et nettoyage
â”‚   â”œâ”€â”€ train/            â† 70% pour l'entraÃ®nement
â”‚   â”œâ”€â”€ val/              â† 20% pour la validation
â”‚   â””â”€â”€ test/             â† 10% pour les tests
â”œâ”€â”€ scripts/              â† Scripts de traitement
â”œâ”€â”€ dataset.yaml          â† Config YOLO (11 classes)
â””â”€â”€ README.md
```

---

## ğŸ§© Ã‰TAPE 2 â€” TÃ©lÃ©charger BDD100K (Jour 1â€“2)

### Pourquoi BDD100K ?
- **100 000 images dashcam** de vraies scÃ¨nes de conduite
- Couvre **11 classes** pertinentes dont les feux tricolores par couleur
- Labels de haute qualitÃ© avec bounding boxes
- C'est **le** dataset de rÃ©fÃ©rence pour la conduite autonome

### TÃ©lÃ©chargement

1. CrÃ©er un compte sur [bdd-data.berkeley.edu](https://bdd-data.berkeley.edu/)
2. TÃ©lÃ©charger :
   - **Images** : `bdd100k_images_100k.zip` (~6.8 Go)
   - **Labels** : Labels de dÃ©tection (format JSON)

### Extraction

Extraire les ZIPs dans `data/bdd100k/` :

```bash
cd autonomous_vision

# CrÃ©er le rÃ©pertoire
mkdir -p data/bdd100k

# Extraire les images et labels dans data/bdd100k/
# Structure attendue aprÃ¨s extraction :
#   data/bdd100k/images/100k/train/*.jpg
#   data/bdd100k/images/100k/val/*.jpg
#   data/bdd100k/labels/det_20/det_train.json
#   data/bdd100k/labels/det_20/det_val.json
```

### Les 11 classes du projet

| ID | Classe | CatÃ©gorie | Source BDD100K |
|----|--------|-----------|----------------|
| 0 | `car` | VÃ©hicule | car âœ… |
| 1 | `truck` | VÃ©hicule | truck âœ… |
| 2 | `bus` | VÃ©hicule | bus âœ… |
| 3 | `motorcycle` | VÃ©hicule | motorcycle âœ… |
| 4 | `bicycle` | VÃ©hicule | bicycle âœ… |
| 5 | `pedestrian` | Usager VulnÃ©rable | pedestrian âœ… |
| 6 | `cyclist` | Usager VulnÃ©rable | rider âœ… |
| 7 | `traffic_light_red` | Signalisation | traffic light (red) âœ… |
| 8 | `traffic_light_green` | Signalisation | traffic light (green) âœ… |
| 9 | `traffic_light_yellow` | Signalisation | traffic light (yellow) âœ… |
| 10 | `traffic_sign` | Signalisation | traffic sign âœ… |

---

## ğŸ§© Ã‰TAPE 3 â€” Convertir en Format YOLO (Jour 2)

### PrÃ©requis

```bash
pip install Pillow
```

### Commande Ã  exÃ©cuter

```bash
cd autonomous_vision

# ğŸ§ª Test rapide (500 images â€” vÃ©rifier que tout fonctionne)
python scripts/convert_bdd100k.py --max-images 500

# ğŸš€ Conversion complÃ¨te (toutes les images)
python scripts/convert_bdd100k.py
```

### Options disponibles

| Option | Description |
|--------|-------------|
| `--max-images 500` | Limiter le nombre d'images (pour tester) |
| `--split train` | Convertir uniquement le split train |
| `--split val` | Convertir uniquement le split validation |
| `--bdd-dir data/bdd100k` | Chemin vers BDD100K (dÃ©faut) |

### Ce que fait le script

1. ğŸ” DÃ©tecte automatiquement la structure des fichiers BDD100K
2. ğŸ“– Lit les labels JSON (supporte le format det_20 et l'ancien format)
3. ğŸ”„ Convertit les coordonnÃ©es `(x1, y1, x2, y2)` â†’ YOLO `(cx, cy, w, h)`
4. ğŸ¨ Classe les feux tricolores par couleur (rouge/vert/jaune)
5. ğŸ“ Sauvegarde dans `data/raw/images/` et `data/raw/labels/`

### Format YOLO (gÃ©nÃ©rÃ© automatiquement)

```
# <class_id> <x_center> <y_center> <width> <height>
# Toutes les valeurs sont normalisÃ©es entre 0 et 1
0 0.4532 0.6210 0.1200 0.2500
5 0.7800 0.5500 0.0400 0.1800
7 0.1200 0.1500 0.0300 0.0600
```

---

## ğŸ§© Ã‰TAPE 4 â€” Filtrer les Classes (Jour 3)

```bash
python scripts/filter_classes.py --raw-dir data/raw --out-dir data/processed
```

Valide les class IDs et copie uniquement les donnÃ©es avec des annotations valides.

---

## ğŸ§© Ã‰TAPE 5 â€” Nettoyer le Dataset (Jour 4)

```bash
# PrÃ©visualiser (sans rien supprimer)
python scripts/clean_dataset.py --data-dir data/processed --dry-run

# Nettoyer
python scripts/clean_dataset.py --data-dir data/processed
```

Supprime : labels vides, images sans label, labels sans image, images corrompues.

---

## ğŸ§© Ã‰TAPE 6 â€” Ã‰quilibrer le Dataset (Jour 5)

```bash
# Analyser (ne modifie rien)
python scripts/balance_dataset.py --data-dir data/processed --analyze-only

# Ã‰quilibrer (augmenter les classes rares)
python scripts/balance_dataset.py --data-dir data/processed --min-objects 1000
```

Duplique et augmente les images des classes sous-reprÃ©sentÃ©es (luminositÃ©, contraste, flou, flip).

---

## ğŸ§© Ã‰TAPE 7 â€” Diviser le Dataset (Jour 6)

```bash
python scripts/split_dataset.py --src-dir data/processed --out-dir data --copy
```

| Split | Pourcentage | RÃ©pertoire |
|-------|-------------|------------|
| Train | 70% | `data/train/` |
| Val | 20% | `data/val/` |
| Test | 10% | `data/test/` |

> [!IMPORTANT]
> Utilisez `--copy` pour garder les donnÃ©es originales dans `data/processed/`.

---

## ğŸ§© Ã‰TAPE 8 â€” EntraÃ®ner avec YOLO (Phase 3)

### Sur Kaggle Notebook (GPU gratuit)

```python
from ultralytics import YOLO

model = YOLO("yolov8s.pt")

results = model.train(
    data="dataset.yaml",
    epochs=100,
    imgsz=640,
    batch=16,
    device=0,
)
```

---

## Phase 3 : EntraÃ®nement du ModÃ¨le (Kaggle)

### Ã‰tape 1 : PrÃ©parer le Dataset pour Kaggle

AprÃ¨s avoir exÃ©cutÃ© tout le pipeline (conversion â†’ filtrage â†’ nettoyage â†’ Ã©quilibrage â†’ split), vous aurez :

```
data/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ images/    (70% des donnÃ©es)
â”‚   â””â”€â”€ labels/
â”œâ”€â”€ val/
â”‚   â”œâ”€â”€ images/    (20% des donnÃ©es)
â”‚   â””â”€â”€ labels/
â””â”€â”€ test/
    â”œâ”€â”€ images/    (10% des donnÃ©es)
    â””â”€â”€ labels/
```

**CrÃ©er un ZIP pour Kaggle :**

```bash
# Compresser le dataset final + dataset.yaml
cd autonomous_vision
zip -r driving-dataset.zip data/train data/val data/test dataset.yaml
```

Ensuite, uploadez `driving-dataset.zip` comme **Kaggle Dataset** sur [kaggle.com/datasets](https://www.kaggle.com/datasets).

### Ã‰tape 2 : EntraÃ®ner sur Kaggle

1. CrÃ©er un **nouveau Notebook** sur Kaggle
2. **Settings** â†’ Accelerator â†’ **GPU T4 x2** (ou P100)
3. **Settings** â†’ Internet â†’ **ON**
4. Ajouter votre dataset au notebook
5. Copier-coller le contenu de `notebooks/kaggle_training.py` dans une cellule

Le script effectue automatiquement :

| Phase | DÃ©tail | Ã‰poques |
|-------|--------|---------|
| **Phase 1** | Transfer Learning (backbone gelÃ©) | 10 |
| **Phase 2** | Fine-Tuning (tous les layers) | 100 |
| **Ã‰valuation** | MÃ©triques sur le test set | â€” |
| **Export** | Conversion en ONNX | â€” |

> âš ï¸ **Sessions Kaggle** : Max **12 heures** par session. Le script sauvegarde des checkpoints toutes les 10 Ã©poques. Pour reprendre :
> ```python
> model = YOLO("/kaggle/input/previous-run/last.pt")
> results = model.train(resume=True)
> ```

### Ã‰tape 3 : RÃ©cupÃ©rer le ModÃ¨le

AprÃ¨s l'entraÃ®nement, cliquez **"Save Version"** â†’ **"Save & Run All"** sur Kaggle pour persister les sorties.

TÃ©lÃ©chargez `best.pt` depuis l'onglet **Output** du notebook et placez-le dans :

```
autonomous_vision/
â””â”€â”€ weights/
    â””â”€â”€ best.pt
```

---

## Phase 3 : InfÃ©rence en Temps RÃ©el

### Lancer le Pipeline

```bash
cd autonomous_vision

# Webcam (camÃ©ra par dÃ©faut)
python -m src.pipeline.realtime_pipeline --model weights/best.pt

# Fichier vidÃ©o
python -m src.pipeline.realtime_pipeline --source driving_video.mp4 --model weights/best.pt

# Avec sauvegarde vidÃ©o
python -m src.pipeline.realtime_pipeline --source video.mp4 --model weights/best.pt --output result.mp4

# Avec estimation de profondeur MiDaS (plus prÃ©cis, plus lent)
python -m src.pipeline.realtime_pipeline --source video.mp4 --model weights/best.pt --midas

# CPU uniquement
python -m src.pipeline.realtime_pipeline --source video.mp4 --model weights/best.pt --device cpu
```

**ContrÃ´les :**
- `Q` ou `ESC` : Quitter
- Le HUD affiche : dÃ©tections, distances, zones de risque, FPS, action en cours

### Options CLI

| Argument | DÃ©faut | Description |
|----------|--------|-------------|
| `--source` | `0` | Fichier vidÃ©o ou index camÃ©ra |
| `--model` | `yolov8s.pt` | Chemin vers les poids YOLO |
| `--config` | `configs` | Dossier de configuration |
| `--device` | `cuda` | `cuda` ou `cpu` |
| `--confidence` | `0.35` | Seuil de confiance |
| `--output` | â€” | Chemin vidÃ©o de sortie |
| `--midas` | off | Activer MiDaS depth |
| `--no-display` | off | DÃ©sactiver l'affichage |

---

## Calibration CamÃ©ra (Optionnel)

Pour amÃ©liorer la prÃ©cision de l'estimation de distance, calibrez votre camÃ©ra :

```bash
# Depuis des images de damier
python scripts/camera_calibration.py --images calibration_images/ --board 9x6

# Depuis la camÃ©ra en direct (appuyer ESPACE pour capturer)
python scripts/camera_calibration.py --camera 0 --board 9x6
```

Les paramÃ¨tres calibrÃ©s sont sauvegardÃ©s dans `configs/camera_params.yaml`.

---

## ğŸ“‹ Checklist RÃ©sumÃ©e

### Phase 2 : Dataset
- [x] CrÃ©er la structure du projet
- [x] CrÃ©er les scripts de traitement
- [x] Configurer `dataset.yaml` (11 classes BDD100K)
- [ ] TÃ©lÃ©charger BDD100K depuis bdd-data.berkeley.edu
- [ ] Extraire les ZIPs dans `data/bdd100k/`
- [ ] Tester la conversion (`convert_bdd100k.py --max-images 500`)
- [ ] Lancer la conversion complÃ¨te (`convert_bdd100k.py`)
- [ ] ExÃ©cuter `filter_classes.py`
- [ ] ExÃ©cuter `clean_dataset.py`
- [ ] ExÃ©cuter `balance_dataset.py`
- [ ] ExÃ©cuter `split_dataset.py`

### Phase 3 : EntraÃ®nement & InfÃ©rence
- [ ] Compresser et uploader le dataset sur Kaggle
- [ ] Lancer `notebooks/kaggle_training.py` sur Kaggle (GPU T4 x2)
- [ ] TÃ©lÃ©charger `best.pt` â†’ `weights/best.pt`
- [ ] Tester : `python -m src.pipeline.realtime_pipeline --model weights/best.pt`
- [ ] (Optionnel) Calibrer la camÃ©ra : `python scripts/camera_calibration.py`

---

*Guide crÃ©Ã© pour le Projet PFE â€” DÃ©tection d'Objets par IA pour la Conduite Autonome*
