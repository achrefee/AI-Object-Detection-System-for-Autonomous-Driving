# üöó Syst√®me de D√©tection d'Objets par IA pour la Conduite Autonome
## D√©tection d'Objets en Temps R√©el, Estimation de Distance & Prise de D√©cision

**Cadre du Projet :** [Ultralytics YOLO](https://github.com/ultralytics/ultralytics)  
**Date :** F√©vrier 2026  
**Statut :** Plan de D√©veloppement & Rapport de Conception du Syst√®me

---

## Table des Mati√®res

1. [Aper√ßu du Projet](#1-aper√ßu-du-projet)
2. [Architecture du Syst√®me](#2-architecture-du-syst√®me)
3. [Classes d'Objets & Strat√©gie de Donn√©es](#3-classes-dobjets--strat√©gie-de-donn√©es)
4. [S√©lection du Mod√®le & Architecture](#4-s√©lection-du-mod√®le--architecture)
5. [Module d'Estimation de Distance](#5-module-destimation-de-distance)
6. [Module de Prise de D√©cision](#6-module-de-prise-de-d√©cision)
7. [Architecture du Pipeline Temps R√©el](#7-architecture-du-pipeline-temps-r√©el)
8. [Plan de Cr√©ation du Dataset](#8-plan-de-cr√©ation-du-dataset)
9. [Strat√©gie d'Entra√Ænement](#9-strat√©gie-dentra√Ænement)
10. [D√©ploiement & Int√©gration Embarqu√©e](#10-d√©ploiement--int√©gration-embarqu√©e)
11. [Phases de D√©veloppement & Calendrier](#11-phases-de-d√©veloppement--calendrier)
12. [Analyse des Risques & Att√©nuation](#12-analyse-des-risques--att√©nuation)
13. [M√©triques d'√âvaluation](#13-m√©triques-d√©valuation)

---

## 1. Aper√ßu du Projet

### 1.1 Probl√©matique

Les v√©hicules autonomes et semi-autonomes n√©cessitent des syst√®mes de perception robustes capables de d√©tecter, classifier et estimer la distance des objets environnants en temps r√©el. Une d√©tection pr√©cise et une estimation de distance fiable sont essentielles pour prendre des d√©cisions de conduite s√ªres telles que le freinage, le changement de voie et l'√©vitement de collision.

### 1.2 Objectifs

| # | Objectif | Priorit√© |
|---|----------|----------|
| 1 | D√©tecter les objets routiers cl√©s (v√©hicules, pi√©tons, panneaux, etc.) en temps r√©el | üî¥ Critique |
| 2 | Estimer la distance de chaque objet d√©tect√© √† l'aide d'une cam√©ra monoculaire | üî¥ Critique |
| 3 | Prendre des d√©cisions de conduite (freiner, acc√©l√©rer, diriger) bas√©es sur les d√©tections | üî¥ Critique |
| 4 | Atteindre ‚â•30 FPS sur du mat√©riel embarqu√© (ex. NVIDIA Jetson) | üü° √âlev√©e |
| 5 | Cr√©er un dataset personnalis√© adapt√© √† l'environnement de conduite cible | üü° √âlev√©e |
| 6 | Atteindre un mAP@0.5 ‚â• 0.85 sur le jeu de test personnalis√© | üü¢ Moyenne |

### 1.3 Stack Technologique

| Composant | Technologie |
|-----------|-------------|
| **D√©tection d'Objets** | Ultralytics YOLOv8 / YOLO11 |
| **Framework d'Apprentissage Profond** | PyTorch ‚â• 2.0 |
| **Plateforme d'Entra√Ænement** | Notebooks Kaggle (GPU Gratuit : NVIDIA Tesla P100 / T4 √ó 2) |
| **Estimation de Distance** | MiDaS / Profondeur Monoculaire Personnalis√©e |
| **Optimisation d'Inf√©rence** | ONNX, TensorRT, OpenVINO |
| **Mat√©riel Embarqu√©** | NVIDIA Jetson Orin / Jetson Xavier NX |
| **Cam√©ra** | Monoculaire RGB (au moins 1080p, 30+ FPS) |
| **Gestion du Dataset** | Roboflow / CVAT / LabelImg |
| **Suivi** | Bot-SORT / ByteTrack |
| **Langage** | Python 3.10+ |

---

## 2. Architecture du Syst√®me

### 2.1 Diagramme d'Architecture de Haut Niveau

```mermaid
graph TB
    subgraph INPUT["üìπ Couche d'Entr√©e"]
        CAM["Flux Cam√©ra<br/>(RGB 1080p @ 30 FPS)"]
    end

    subgraph PERCEPTION["üß† Couche de Perception"]
        PREPROCESS["Pr√©traitement d'Image<br/>(Redimensionnement, Normalisation, Letterbox)"]
        DETECTION["D√©tection d'Objets YOLOv8<br/>(Bo√Ætes Englobantes + Classes + Confiance)"]
        TRACKING["Suivi d'Objets<br/>(Bot-SORT / ByteTrack)"]
        DEPTH["Estimation de Distance<br/>(Profondeur Monoculaire / G√©om√©trie)"]
    end

    subgraph DECISION["‚ö° Couche de D√©cision"]
        RISK["√âvaluation des Risques<br/>(TTC, Zones de Proximit√©)"]
        PLANNER["Planificateur d'Actions<br/>(Freiner / Acc√©l√©rer / Diriger)"]
    end

    subgraph OUTPUT["üöò Couche de Sortie"]
        ACTUATOR["Interface de Contr√¥le V√©hicule<br/>(Bus CAN / S√©rie)"]
        DISPLAY["Affichage Conducteur<br/>(Superposition HUD)"]
        LOGGER["Enregistreur de Donn√©es<br/>(T√©l√©m√©trie + √âv√©nements)"]
    end

    CAM --> PREPROCESS
    PREPROCESS --> DETECTION
    DETECTION --> TRACKING
    DETECTION --> DEPTH
    TRACKING --> RISK
    DEPTH --> RISK
    RISK --> PLANNER
    PLANNER --> ACTUATOR
    PLANNER --> DISPLAY
    PLANNER --> LOGGER

    style INPUT fill:#1a1a2e,stroke:#00d4ff,color:#fff
    style PERCEPTION fill:#16213e,stroke:#0f3460,color:#fff
    style DECISION fill:#0f3460,stroke:#e94560,color:#fff
    style OUTPUT fill:#533483,stroke:#e94560,color:#fff
```

### 2.2 Diagramme d'Interaction des Composants

```mermaid
sequenceDiagram
    participant Cam√©ra
    participant Pr√©processeur
    participant D√©tecteurYOLO
    participant Suiveur
    participant EstimateurProfondeur
    participant √âvaluateurRisque
    participant PlanificateurAction
    participant V√©hicule

    loop Chaque Image (30+ FPS)
        Cam√©ra->>Pr√©processeur: Image RGB Brute
        Pr√©processeur->>D√©tecteurYOLO: Tenseur Redimensionn√© & Normalis√©
        D√©tecteurYOLO->>Suiveur: D√©tections (bbox, classe, conf)
        D√©tecteurYOLO->>EstimateurProfondeur: D√©tections + Image Originale
        Suiveur->>√âvaluateurRisque: Objets Suivis (ID, vitesse, trajectoire)
        EstimateurProfondeur->>√âvaluateurRisque: Distance par Objet
        √âvaluateurRisque->>PlanificateurAction: Carte de Risque (objet, distance, TTC)
        PlanificateurAction->>V√©hicule: Signal de Contr√¥le (freiner, diriger, vitesse)
    end
```

---

## 3. Classes d'Objets & Strat√©gie de Donn√©es

### 3.1 Classes d'Objets Cibles

Le syst√®me doit d√©tecter les cat√©gories suivantes, organis√©es par priorit√© :

```mermaid
mindmap
  root((Objets<br/>D√©tectables))
    üöó V√©hicules
      Voiture
      Camion
      Bus
      Moto
      V√©lo
    üö∂ Usagers Vuln√©rables
      Pi√©ton
      Cycliste
      Enfant
    üö¶ Infrastructure Routi√®re
      Feu Rouge
      Feu Vert
      Feu Orange
      Panneau Stop
      Panneau de Limitation de Vitesse
      Panneau C√©dez le Passage
      Panneau Sens Interdit
    ‚ö†Ô∏è Obstacles Routiers
      Barri√®re Routi√®re
      C√¥ne
      Nid-de-poule
      Animal
    üõ§Ô∏è √âl√©ments de Route
      Marquage au Sol
      Passage Pi√©ton
      Bord de Route
```

### 3.2 Tableau Complet des Classes

| ID | Nom de Classe | Cat√©gorie | Priorit√© | √âchantillons Estim√©s N√©cessaires |
|----|--------------|-----------|----------|----------------------------------|
| 0 | `car` | V√©hicule | üî¥ Critique | 5 000+ |
| 1 | `truck` | V√©hicule | üî¥ Critique | 3 000+ |
| 2 | `bus` | V√©hicule | üü° √âlev√©e | 2 000+ |
| 3 | `motorcycle` | V√©hicule | üü° √âlev√©e | 2 000+ |
| 4 | `bicycle` | V√©hicule | üü° √âlev√©e | 2 000+ |
| 5 | `pedestrian` | UVR | üî¥ Critique | 5 000+ |
| 6 | `cyclist` | UVR | üî¥ Critique | 3 000+ |
| 7 | `traffic_light_red` | Signalisation | üî¥ Critique | 3 000+ |
| 8 | `traffic_light_green` | Signalisation | üî¥ Critique | 3 000+ |
| 9 | `traffic_light_yellow` | Signalisation | üü° √âlev√©e | 2 000+ |
| 10 | `stop_sign` | Signalisation | üî¥ Critique | 2 000+ |
| 11 | `speed_limit_sign` | Signalisation | üü° √âlev√©e | 2 000+ |
| 12 | `yield_sign` | Signalisation | üü° √âlev√©e | 1 500+ |
| 13 | `no_entry_sign` | Signalisation | üü° √âlev√©e | 1 500+ |
| 14 | `road_barrier` | Obstacle | üü¢ Moyenne | 1 500+ |
| 15 | `cone` | Obstacle | üü¢ Moyenne | 1 500+ |
| 16 | `pothole` | Obstacle | üü¢ Moyenne | 1 000+ |
| 17 | `crosswalk` | Route | üü¢ Moyenne | 1 500+ |

**Total : 18 classes | ~45 000+ images annot√©es recommand√©es**

### 3.3 Sources de Donn√©es & Strat√©gie de Construction

```mermaid
graph LR
    subgraph PUBLIC["üì¶ Datasets Publics"]
        KITTI["KITTI<br/>(7 481 imgs, v√©hicules, pi√©tons)"]
        BDD["BDD100K<br/>(100K vid√©os, conditions vari√©es)"]
        COCO["COCO<br/>(330K imgs, 80 classes)"]
        GTSRB["GTSRB<br/>(50K+ panneaux de signalisation)"]
        MAPILLARY["Mapillary Traffic Signs<br/>(100K+ panneaux mondiaux)"]
    end

    subgraph CUSTOM["üì∏ Collection Personnalis√©e"]
        OWN_CAM["Enregistrement Cam√©ra<br/>(Routes locales & autoroutes)"]
        DASHCAM["Images Dashcam<br/>(YouTube, Sources Ouvertes)"]
        SYNTH["Donn√©es Synth√©tiques<br/>(Simulateur CARLA)"]
    end

    subgraph AUGMENT["üîÑ Augmentation"]
        FLIP["Retournement Horizontal"]
        BRIGHTNESS["Variation de Luminosit√©"]
        BLUR["Flou de Mouvement"]
        WEATHER["Superposition M√©t√©o<br/>(Pluie, Brouillard, Nuit)"]
        MOSAIC["Augmentation Mosa√Øque"]
        MIXUP["MixUp"]
    end

    subgraph FINAL["‚úÖ Dataset Final"]
        MERGED["Fusionn√© & Nettoy√©<br/>Format YOLO"]
    end

    PUBLIC --> MERGED
    CUSTOM --> MERGED
    MERGED --> AUGMENT
    AUGMENT --> FINAL

    style PUBLIC fill:#1b4332,stroke:#52b788,color:#fff
    style CUSTOM fill:#003049,stroke:#669bbc,color:#fff
    style AUGMENT fill:#6a040f,stroke:#e85d04,color:#fff
    style FINAL fill:#3c096c,stroke:#c77dff,color:#fff
```

---

## 4. S√©lection du Mod√®le & Architecture

### 4.1 Aper√ßu de l'Architecture YOLOv8

```mermaid
graph LR
    subgraph BACKBONE["üîß Backbone (CSPDarknet)"]
        INPUT_IMG["Image d'Entr√©e<br/>(640√ó640√ó3)"]
        CONV1["Bloc Conv"]
        C2F1["Bloc C2f √ó 3"]
        SPPF["SPPF<br/>(Spatial Pyramid Pooling Fast)"]
    end

    subgraph NECK["üîó Cou (PANet / FPN)"]
        UPSAMPLE1["Sur√©chantillonnage"]
        CONCAT1["Concat√©nation"]
        C2F2["Bloc C2f"]
        UPSAMPLE2["Sur√©chantillonnage"]
        CONCAT2["Concat√©nation"]
        C2F3["Bloc C2f"]
    end

    subgraph HEAD["üéØ T√™te de D√©tection (D√©coupl√©e)"]
        CLS["Branche Classification<br/>(18 classes)"]
        REG["Branche R√©gression<br/>(Bo√Æte Englobante DFL)"]
    end

    INPUT_IMG --> CONV1
    CONV1 --> C2F1
    C2F1 --> SPPF
    SPPF --> UPSAMPLE1
    UPSAMPLE1 --> CONCAT1
    CONCAT1 --> C2F2
    C2F2 --> UPSAMPLE2
    UPSAMPLE2 --> CONCAT2
    CONCAT2 --> C2F3
    C2F3 --> CLS
    C2F3 --> REG

    style BACKBONE fill:#1a1a2e,stroke:#e94560,color:#fff
    style NECK fill:#16213e,stroke:#0f3460,color:#fff
    style HEAD fill:#0f3460,stroke:#00d4ff,color:#fff
```

### 4.2 Comparaison des Variantes de Mod√®le

| Mod√®le | Params (M) | mAP@0.5 (COCO) | Vitesse GPU (ms) | Utilisation Recommand√©e |
|--------|-----------|-----------------|-------------------|-------------------------|
| **YOLOv8n** | 3.2 | 37.3 | 1.2 | Appareils embarqu√©s, vitesse max |
| **YOLOv8s** | 11.2 | 44.9 | 1.7 | ‚úÖ **Meilleur √©quilibre pour la conduite** |
| **YOLOv8m** | 25.9 | 50.2 | 3.4 | Haute pr√©cision, bons GPUs |
| **YOLOv8l** | 43.7 | 52.9 | 5.3 | Inf√©rence cloud/serveur |
| **YOLOv8x** | 68.2 | 53.9 | 7.8 | Pr√©cision maximale |

> [!IMPORTANT]
> **Recommand√© :** Commencer avec **YOLOv8s** pour le meilleur compromis entre vitesse (‚â•30 FPS sur Jetson) et pr√©cision. Si vous utilisez un GPU puissant (RTX 3060+), envisagez **YOLOv8m**.

### 4.3 Innovations Architecturales Cl√©s Utilis√©es

| Fonctionnalit√© | Description |
|----------------|-------------|
| **Module C2f** | Cross Stage Partial avec caract√©ristiques fines pour un flux de gradient plus riche |
| **T√™te D√©coupl√©e** | Branches de classification et de r√©gression s√©par√©es pour une meilleure convergence |
| **Sans Ancres** | √âlimine les bo√Ætes d'ancrage manuelles ; pr√©dit directement les centres d'objets |
| **Perte DFL** | Distribution Focal Loss pour une r√©gression pr√©cise des bo√Ætes englobantes |
| **Augmentation Mosa√Øque** | Combine 4 images pour apprendre les petits objets et les contextes vari√©s |

---

## 5. Module d'Estimation de Distance

### 5.1 Comparaison des Approches

| M√©thode | Pr√©cision | Vitesse | Mat√©riel | Complexit√© |
|---------|-----------|---------|----------|------------|
| **G√©om√©trie de Bo√Æte Englobante** | ‚òÖ‚òÖ‚òÜ‚òÜ‚òÜ | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ | Cam√©ra Unique | Faible |
| **Profondeur Monoculaire (MiDaS)** | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ | ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ | Cam√©ra Unique + GPU | Moyenne |
| **Vision St√©r√©o** | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ | ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ | Double Cam√©ra | √âlev√©e |
| **Fusion LiDAR** | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ | LiDAR + Cam√©ra | Tr√®s √âlev√©e |

### 5.2 Approche Recommand√©e : Estimation Hybride de Distance Monoculaire

Nous utilisons une **approche hybride en deux √©tapes** combinant la g√©om√©trie de bo√Æte englobante (rapide) avec l'estimation de profondeur monoculaire (pr√©cise) :

```mermaid
graph TB
    subgraph STAGE1["‚ö° √âtape 1 : Estimation G√©om√©trique Rapide"]
        BBOX["Bo√Æte Englobante YOLO<br/>(x, y, w, h)"]
        KNOWN["Hauteurs Connues des Objets<br/>(Voiture: 1.5m, Camion: 3.5m, Pi√©ton: 1.7m)"]
        FOCAL["Longueur Focale de la Cam√©ra<br/>(Calibr√©e)"]
        CALC["Distance = (Hauteur R√©elle √ó Longueur Focale) <br/> √∑ Hauteur Bo√Æte Englobante (px)"]
    end

    subgraph STAGE2["üß† √âtape 2 : Raffinement de Profondeur (MiDaS)"]
        FRAME["Image Compl√®te"]
        MIDAS["MiDaS v3.1 DPT<br/>(Profondeur Monoculaire)"]
        DEPTHMAP["Carte de Profondeur Dense"]
        SAMPLE["√âchantillonnage de Profondeur au<br/>Centre de l'Objet"]
    end

    subgraph FUSION["üîó Fusion"]
        WEIGHTED["Moyenne Pond√©r√©e<br/>(Œ± √ó G√©om√©trique + Œ≤ √ó MiDaS)"]
        KALMAN["Filtre de Kalman<br/>(Lissage Temporel)"]
        FINAL_DIST["Estimation Finale<br/>de Distance (m√®tres)"]
    end

    BBOX --> CALC
    KNOWN --> CALC
    FOCAL --> CALC
    FRAME --> MIDAS --> DEPTHMAP --> SAMPLE
    CALC --> WEIGHTED
    SAMPLE --> WEIGHTED
    WEIGHTED --> KALMAN --> FINAL_DIST

    style STAGE1 fill:#1b4332,stroke:#52b788,color:#fff
    style STAGE2 fill:#003049,stroke:#669bbc,color:#fff
    style FUSION fill:#3c096c,stroke:#c77dff,color:#fff
```

### 5.3 Param√®tres de Calibration de la Cam√©ra

```
Matrice Intrins√®que K :
‚îå              ‚îê
‚îÇ fx  0   cx   ‚îÇ
‚îÇ 0   fy  cy   ‚îÇ
‚îÇ 0   0   1    ‚îÇ
‚îî              ‚îò

O√π :
  fx, fy = Longueur focale (pixels)
  cx, cy = Point principal (centre de l'image)

Formule de Distance (Mod√®le St√©nop√©) :
  D = (H_r√©el √ó f_y) / h_bbox

O√π :
  D       = Distance √† l'objet (m√®tres)
  H_r√©el  = Hauteur r√©elle connue de l'objet (m√®tres)
  f_y     = Longueur focale en direction y (pixels)
  h_bbox  = Hauteur de la bo√Æte englobante dans l'image (pixels)
```

### 5.4 Dimensions Connues des Objets (Table de R√©f√©rence)

| Classe d'Objet | Hauteur Moy. (m) | Largeur Moy. (m) | Longueur Moy. (m) |
|----------------|-------------------|-------------------|---------------------|
| Voiture | 1.50 | 1.80 | 4.50 |
| Camion | 3.50 | 2.50 | 12.00 |
| Bus | 3.20 | 2.50 | 12.00 |
| Moto | 1.10 | 0.80 | 2.10 |
| V√©lo | 1.00 | 0.60 | 1.80 |
| Pi√©ton | 1.70 | 0.50 | 0.30 |
| Feu de Signalisation | 0.40 | 0.30 | 0.20 |
| Panneau Stop | 0.75 | 0.75 | ‚Äî |

---

## 6. Module de Prise de D√©cision

### 6.1 Classification des Zones de Risque

```mermaid
graph LR
    subgraph ZONES["Zones de Proximit√©"]
        CRIT["üî¥ CRITIQUE<br/>0‚Äì5 m√®tres<br/>FREINAGE D'URGENCE"]
        DANGER["üü† DANGER<br/>5‚Äì15 m√®tres<br/>FREINAGE FORT / DIRECTION"]
        WARNING["üü° AVERTISSEMENT<br/>15‚Äì30 m√®tres<br/>RALENTIR"]
        SAFE["üü¢ S√õR<br/>30+ m√®tres<br/>MAINTENIR LA VITESSE"]
    end

    CRIT --> DANGER --> WARNING --> SAFE

    style CRIT fill:#d00000,stroke:#370617,color:#fff
    style DANGER fill:#e85d04,stroke:#6a040f,color:#fff
    style WARNING fill:#faa307,stroke:#6a040f,color:#000
    style SAFE fill:#2d6a4f,stroke:#1b4332,color:#fff
```

### 6.2 Logique de l'Arbre de D√©cision

```mermaid
flowchart TD
    START["Nouvelle Image de D√©tection"] --> CHECK{"Objets<br/>D√©tect√©s ?"}
    CHECK -->|Non| MAINTAIN["‚úÖ Maintenir la Vitesse Actuelle"]
    CHECK -->|Oui| CLASSIFY["Classifier Chaque Objet<br/>(Classe + Distance + Vitesse)"]
    
    CLASSIFY --> TTC{"Calcul du Temps<br/>Avant Collision (TTC)"}
    
    TTC --> TTC_CRIT{"TTC < 1.5s ?"}
    TTC_CRIT -->|Oui| EMERGENCY["üî¥ FREINAGE D'URGENCE<br/>Activation ABS Compl√®te"]
    TTC_CRIT -->|Non| TTC_WARN{"TTC < 3.0s ?"}
    
    TTC_WARN -->|Oui| ZONE_CHECK{"Objet dans la<br/>Voie Ego ?"}
    ZONE_CHECK -->|Oui| HARD_BRAKE["üü† FREINAGE FORT<br/>+ V√©rif. Changement de Voie"]
    ZONE_CHECK -->|Non| MONITOR["üü° RALENTIR<br/>+ Surveiller"]
    
    TTC_WARN -->|Non| DIST_CHECK{"Distance<br/>< 30m ?"}
    DIST_CHECK -->|Oui| CAUTION["üü° R√âDUIRE LA VITESSE<br/>Augmenter la Distance de Suivi"]
    DIST_CHECK -->|Non| MAINTAIN2["‚úÖ MAINTENIR LA VITESSE"]

    HARD_BRAKE --> LANE{"Voie S√ªre<br/>Disponible ?"}
    LANE -->|Oui| STEER["‚ÜîÔ∏è CHANGEMENT DE VOIE"]
    LANE -->|Non| BRAKE_ONLY["üõë FREINAGE UNIQUEMENT"]

    style EMERGENCY fill:#d00000,color:#fff
    style HARD_BRAKE fill:#e85d04,color:#fff
    style MONITOR fill:#faa307,color:#000
    style CAUTION fill:#faa307,color:#000
    style MAINTAIN fill:#2d6a4f,color:#fff
    style MAINTAIN2 fill:#2d6a4f,color:#fff
    style STEER fill:#003049,color:#fff
    style BRAKE_ONLY fill:#6a040f,color:#fff
```

### 6.3 Formule du Temps Avant Collision (TTC)

```
TTC = Distance / Vitesse_Relative

O√π :
  Distance          = Distance estim√©e √† l'objet (m√®tres)
  Vitesse_Relative  = (V_ego - V_objet) en m/s
  
  Si Vitesse_Relative ‚â§ 0 ‚Üí TTC = ‚àû (l'objet s'√©loigne ou m√™me vitesse)
```

### 6.4 Logique de D√©cision pour les Feux de Signalisation

```mermaid
flowchart LR
    TL["Feu de<br/>Signalisation D√©tect√©"] --> COLOR{"Couleur ?"}
    COLOR -->|Rouge| STOP["üõë ARR√äT<br/>Avant l'intersection"]
    COLOR -->|Orange| ASSESS{"Distance √†<br/>l'Intersection ?"}
    ASSESS -->|Proche| PROCEED["‚ö†Ô∏è Continuer<br/>avec prudence"]
    ASSESS -->|Loin| SLOW["üü° Commencer<br/>√† ralentir"]
    COLOR -->|Vert| GO["‚úÖ CONTINUER<br/>√† la vitesse actuelle"]

    style STOP fill:#d00000,color:#fff
    style SLOW fill:#faa307,color:#000
    style PROCEED fill:#e85d04,color:#fff
    style GO fill:#2d6a4f,color:#fff
```

---

## 7. Architecture du Pipeline Temps R√©el

### 7.1 Pipeline de Traitement (Par Image)

```mermaid
gantt
    title Pipeline de Traitement d'Image (~33ms budget @ 30 FPS)
    dateFormat X
    axisFormat %Lms

    section Capture
    Capture Cam√©ra          :cam, 0, 2

    section Pr√©traitement
    Redimensionnement + Norm :pre, 2, 4

    section D√©tection
    Inf√©rence YOLO (GPU)    :det, 4, 14

    section Suivi
    Mise √† jour Bot-SORT    :track, 14, 17

    section Distance
    Estimation de Profondeur :depth, 14, 22

    section D√©cision
    √âvaluation des Risques   :risk, 22, 26
    Planification d'Action   :plan, 26, 29

    section Sortie
    Affichage + Actionneur   :out, 29, 33
```

### 7.2 Architecture Multi-Thread

```mermaid
graph TB
    subgraph THREAD1["Thread 1 : Capture"]
        T1["Capture d'Image Cam√©ra<br/>(Tampon Circulaire)"]
    end

    subgraph THREAD2["Thread 2 : D√©tection"]
        T2A["Pr√©traitement Image"]
        T2B["Inf√©rence YOLO<br/>(GPU)"]
        T2C["Post-traitement<br/>(NMS)"]
    end

    subgraph THREAD3["Thread 3 : Profondeur"]
        T3["Estimation de Profondeur<br/>MiDaS (GPU)"]
    end

    subgraph THREAD4["Thread 4 : D√©cision"]
        T4A["Suivi d'Objets"]
        T4B["√âvaluation des Risques"]
        T4C["Planification d'Action"]
    end

    subgraph THREAD5["Thread 5 : Sortie"]
        T5A["Superposition HUD"]
        T5B["Commandes Bus CAN"]
        T5C["Journalisation"]
    end

    T1 -->|File d'Images| T2A
    T2A --> T2B --> T2C
    T1 -->|File d'Images| T3
    T2C -->|D√©tections| T4A
    T3 -->|Carte de Prof.| T4B
    T4A --> T4B --> T4C
    T4C --> T5A
    T4C --> T5B
    T4C --> T5C

    style THREAD1 fill:#1a1a2e,stroke:#e94560,color:#fff
    style THREAD2 fill:#16213e,stroke:#0f3460,color:#fff
    style THREAD3 fill:#0f3460,stroke:#00d4ff,color:#fff
    style THREAD4 fill:#3c096c,stroke:#c77dff,color:#fff
    style THREAD5 fill:#533483,stroke:#e94560,color:#fff
```

---

## 8. Plan de Cr√©ation du Dataset

### 8.1 Composition du Dataset

```mermaid
pie title Distribution des Sources du Dataset (Pr√©vu ~50K images)
    "KITTI (v√©hicules, pi√©tons)" : 15
    "BDD100K (conduite vari√©e)" : 25
    "COCO (classes filtr√©es)" : 10
    "GTSRB (panneaux de signalisation)" : 15
    "Collection Personnalis√©e" : 20
    "Synth√©tique (CARLA)" : 10
    "Copies Augment√©es" : 5
```

### 8.2 Processus de Cr√©ation du Dataset √âtape par √âtape

#### √âtape 1 : Collecter et T√©l√©charger les Datasets Publics

| Dataset | Source | Classes Utilis√©es | Format |
|---------|--------|-------------------|--------|
| **KITTI** | [cvlibs.net/datasets/kitti](http://www.cvlibs.net/datasets/kitti/) | Voiture, Camion, Pi√©ton, Cycliste | Format KITTI ‚Üí convertir en YOLO |
| **BDD100K** | [bdd-data.berkeley.edu](https://bdd-data.berkeley.edu/) | Tous types de v√©hicules, pi√©tons, feux | JSON ‚Üí convertir en YOLO |
| **COCO 2017** | [cocodataset.org](https://cocodataset.org/) | voiture, camion, bus, moto, v√©lo, personne, feu, panneau stop | COCO JSON ‚Üí convertir en YOLO |
| **GTSRB** | [benchmark.ini.rub.de](https://benchmark.ini.rub.de/) | Limitations de vitesse, stop, c√©dez le passage, sens interdit | Classification ‚Üí cr√©er labels de d√©tection |
| **Mapillary Traffic Signs** | [mapillary.com/dataset/trafficsign](https://www.mapillary.com/dataset/trafficsign) | Panneaux de signalisation mondiaux | Convertir en YOLO |

#### √âtape 2 : Collection de Donn√©es Personnalis√©es

```
Configuration d'Enregistrement :
  ‚îú‚îÄ‚îÄ Cam√©ra : Dashcam ou cam√©ra IP (1080p, 30 FPS, grand angle)
  ‚îú‚îÄ‚îÄ Montage : Centre du tableau de bord, orient√©e vers l'avant
  ‚îú‚îÄ‚îÄ Dur√©e d'Enregistrement : 20+ heures de conduite vari√©e
  ‚îî‚îÄ‚îÄ Sc√©narios √† Couvrir :
       ‚îú‚îÄ‚îÄ Conduite urbaine (intersections, pi√©tons)
       ‚îú‚îÄ‚îÄ Conduite sur autoroute (haute vitesse, camions, changements de voie)
       ‚îú‚îÄ‚îÄ Routes de banlieue (r√©sidentiel, √©coles, parcs)
       ‚îú‚îÄ‚îÄ Conduite de nuit (phares, faible visibilit√©)
       ‚îú‚îÄ‚îÄ Conditions de pluie/brouillard
       ‚îî‚îÄ‚îÄ Zones de travaux (c√¥nes, barri√®res)
```

#### √âtape 3 : Pipeline d'Annotation

```mermaid
flowchart LR
    RAW["Images Brutes<br/>(Collect√©es + T√©l√©charg√©es)"] 
    --> FILTER["Filtrer & S√©lectionner<br/>(Supprimer doublons,<br/>flou, mauvaise qualit√©)"]
    --> ANNOTATE["Annoter avec CVAT<br/>(Bo√Ætes Englobantes<br/>+ √âtiquettes de Classe)"]
    --> REVIEW["Revue de Qualit√©<br/>(V√©rification crois√©e,<br/>correction d'erreurs)"]
    --> CONVERT["Convertir en Format<br/>YOLO<br/>(fichiers txt)"]
    --> SPLIT["Division Train/Val/Test<br/>(70/20/10)"]
    --> FINAL_DS["Dataset Final<br/>Pr√™t pour l'Entra√Ænement"]

    style RAW fill:#003049,color:#fff
    style FILTER fill:#1b4332,color:#fff
    style ANNOTATE fill:#6a040f,color:#fff
    style REVIEW fill:#e85d04,color:#fff
    style CONVERT fill:#3c096c,color:#fff
    style SPLIT fill:#0f3460,color:#fff
    style FINAL_DS fill:#2d6a4f,color:#fff
```

#### √âtape 4 : Structure du Format YOLO

```
dataset/
‚îú‚îÄ‚îÄ data.yaml                   # Configuration du dataset
‚îú‚îÄ‚îÄ train/
‚îÇ   ‚îú‚îÄ‚îÄ images/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ img_00001.jpg
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ img_00002.jpg
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ labels/
‚îÇ       ‚îú‚îÄ‚îÄ img_00001.txt       # <id_classe> <x_centre> <y_centre> <largeur> <hauteur>
‚îÇ       ‚îú‚îÄ‚îÄ img_00002.txt
‚îÇ       ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ val/
‚îÇ   ‚îú‚îÄ‚îÄ images/
‚îÇ   ‚îî‚îÄ‚îÄ labels/
‚îî‚îÄ‚îÄ test/
    ‚îú‚îÄ‚îÄ images/
    ‚îî‚îÄ‚îÄ labels/
```

**Format d'√âtiquette YOLO** (normalis√© 0‚Äì1) :
```
# <id_classe> <x_centre> <y_centre> <largeur> <hauteur>
0 0.4532 0.6210 0.1200 0.2500
5 0.7800 0.5500 0.0400 0.1800
7 0.2100 0.3000 0.0250 0.0600
```

#### √âtape 5 : Configuration data.yaml

```yaml
# data.yaml - Configuration du dataset pour Ultralytics YOLO
path: ./dataset
train: train/images
val: val/images
test: test/images

# Nombre de classes
nc: 18

# Noms des classes
names:
  0: car
  1: truck
  2: bus
  3: motorcycle
  4: bicycle
  5: pedestrian
  6: cyclist
  7: traffic_light_red
  8: traffic_light_green
  9: traffic_light_yellow
  10: stop_sign
  11: speed_limit_sign
  12: yield_sign
  13: no_entry_sign
  14: road_barrier
  15: cone
  16: pothole
  17: crosswalk
```

### 8.3 Strat√©gie d'Augmentation de Donn√©es

| Augmentation | Param√®tre | Objectif |
|-------------|-----------|----------|
| **Retournement Horizontal** | p=0.5 | Variations de conduite gauche/droite |
| **D√©calage de Teinte HSV** | ¬±15¬∞ | Robustesse aux couleurs |
| **Saturation HSV** | ¬±40% | Variations d'√©clairage |
| **Valeur HSV** | ¬±40% | Robustesse √† la luminosit√© |
| **Mosa√Øque** | p=1.0 | Apprentissage multi-√©chelle, petits objets |
| **MixUp** | p=0.15 | R√©gularisation |
| **Copier-Coller** | p=0.1 | Augmentation des classes rares |
| **Perspective** | ¬±0.001 | Variations de point de vue |
| **Flou de Mouvement** | noyau=5 | Simuler un mouvement rapide |
| **Superposition Pluie/Brouillard** | Personnalis√© | Robustesse aux conditions m√©t√©o d√©favorables |

---

## 9. Strat√©gie d'Entra√Ænement

### 9.1 Pipeline d'Entra√Ænement

```mermaid
flowchart TD
    subgraph PHASE1["Phase 1 : Apprentissage par Transfert"]
        P1A["Charger YOLOv8s Pr√©-entra√Æn√©<br/>(Poids COCO)"]
        P1B["Geler le Backbone<br/>(10 √©poques)"]
        P1C["Entra√Æner la T√™te Uniquement<br/>(lr=0.01, batch=16)"]
    end

    subgraph PHASE2["Phase 2 : Affinage"]
        P2A["D√©geler Toutes les Couches"]
        P2B["R√©duire le LR (lr=0.001)"]
        P2C["Entra√Æner le Mod√®le Complet<br/>(100 √©poques, patience=20)"]
    end

    subgraph PHASE3["Phase 3 : Optimisation"]
        P3A["R√©glage des Hyperparam√®tres<br/>(Ultralytics Ray Tune)"]
        P3B["Exporter en ONNX"]
        P3C["Convertir en TensorRT<br/>(FP16 / INT8)"]
    end

    subgraph PHASE4["Phase 4 : Validation"]
        P4A["√âvaluer sur le Jeu de Test"]
        P4B["Test Vid√©o R√©el"]
        P4C["Benchmark Appareil Embarqu√©"]
    end

    PHASE1 --> PHASE2 --> PHASE3 --> PHASE4

    style PHASE1 fill:#1b4332,stroke:#52b788,color:#fff
    style PHASE2 fill:#003049,stroke:#669bbc,color:#fff
    style PHASE3 fill:#6a040f,stroke:#e85d04,color:#fff
    style PHASE4 fill:#3c096c,stroke:#c77dff,color:#fff
```

### 9.2 Environnement d'Entra√Ænement Kaggle

> [!IMPORTANT]
> **Plateforme d'Entra√Ænement :** Nous utilisons les **Notebooks Kaggle** avec acc√©l√©rateurs GPU gratuits.
> - **Options GPU :** NVIDIA Tesla P100 (16 Go) ou T4 √ó 2 (2 √ó 16 Go)
> - **Limite de Session :** 30 heures/semaine de GPU, session max de 12 heures
> - **Disque :** 20 Go persistant + 70 Go temporaire
> - **RAM :** 13 Go (CPU) / 13 Go (mode GPU)
> - **Int√©gration Dataset :** Les Datasets Kaggle sont mont√©s √† `/kaggle/input/`

#### Configuration du Notebook Kaggle

```python
# ============================================================
# NOTEBOOK KAGGLE ‚Äî Entra√Ænement YOLO pour la Conduite Autonome
# ============================================================
# Param√®tres ‚Üí Acc√©l√©rateur ‚Üí GPU T4 x2 (ou P100)
# Param√®tres ‚Üí Internet ‚Üí ACTIV√â (pour t√©l√©charger les poids pr√©-entra√Æn√©s)
# ============================================================

# √âtape 1 : Installer Ultralytics (pr√©-install√© sur Kaggle, mettre √† jour)
!pip install -q ultralytics --upgrade

# √âtape 2 : V√©rifier la disponibilit√© du GPU
import torch
print(f"GPU Disponible : {torch.cuda.is_available()}")
print(f"Nom du GPU : {torch.cuda.get_device_name(0)}")
print(f"M√©moire GPU : {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} Go")

# √âtape 3 : Lier au Dataset Kaggle
# T√©l√©chargez votre dataset en tant que Dataset Kaggle, puis ajoutez-le au notebook.
# Il sera disponible √† : /kaggle/input/<nom-du-dataset>/
import os
DATASET_PATH = "/kaggle/input/driving-object-detection"  # Votre dataset Kaggle
OUTPUT_PATH = "/kaggle/working"                           # R√©pertoire de sortie
```

#### Configuration d'Entra√Ænement (Optimis√©e pour Kaggle)

```python
from ultralytics import YOLO

# Phase 1 : Apprentissage par Transfert sur Kaggle
model = YOLO("yolov8s.pt")  # T√©l√©chargement auto des poids COCO pr√©-entra√Æn√©s

results = model.train(
    data=f"{DATASET_PATH}/data.yaml",
    epochs=100,
    imgsz=640,
    batch=16,              # Adapt√© pour 16 Go VRAM P100/T4
    patience=20,
    optimizer="AdamW",
    lr0=0.01,
    lrf=0.01,              # Facteur de taux d'apprentissage final
    momentum=0.937,
    weight_decay=0.0005,
    warmup_epochs=3,
    warmup_momentum=0.8,
    warmup_bias_lr=0.1,
    
    # Augmentation
    hsv_h=0.015,           # Augmentation de teinte
    hsv_s=0.7,             # Augmentation de saturation
    hsv_v=0.4,             # Augmentation de valeur
    degrees=0.0,           # Rotation
    translate=0.1,         # Translation
    scale=0.5,             # √âchelle
    fliplr=0.5,            # Retournement horizontal
    mosaic=1.0,            # Augmentation mosa√Øque
    mixup=0.15,            # Augmentation MixUp
    copy_paste=0.1,        # Augmentation Copier-Coller
    
    # Mat√©riel ‚Äî GPU Kaggle
    device=0,              # GPU 0 (P100 ou T4)
    workers=2,             # Kaggle a peu de c≈ìurs CPU
    
    # Sauvegarde ‚Äî sortie vers /kaggle/working/ (t√©l√©chargeable)
    project=f"{OUTPUT_PATH}/runs/train",
    name="driving_detector_v1",
    save=True,
    save_period=10,        # Point de contr√¥le toutes les 10 √©poques
    plots=True,
)

# √âtape 4 : T√©l√©charger les meilleurs poids apr√®s l'entra√Ænement
# Le meilleur mod√®le sera sauvegard√© √† :
# /kaggle/working/runs/train/driving_detector_v1/weights/best.pt
# ‚Üí Cliquez "Save Version" ‚Üí "Save & Run All" pour conserver les sorties
print(f"Meilleur mod√®le sauvegard√© √† : {OUTPUT_PATH}/runs/train/driving_detector_v1/weights/best.pt")
```

### 9.3 Conseils de Gestion de Session Kaggle

> [!WARNING]
> Les sessions Kaggle expirent apr√®s **12 heures maximum**. Planifiez votre strat√©gie d'entra√Ænement en cons√©quence :

| Conseil | Description |
|---------|-------------|
| **Utiliser `save_period=10`** | Sauvegarder les points de contr√¥le toutes les 10 √©poques pour reprendre si la session expire |
| **Reprendre l'entra√Ænement** | Utiliser `model = YOLO("last.pt")` puis `model.train(resume=True)` pour continuer |
| **Diviser l'entra√Ænement** | Phase 1 (backbone gel√©, 10 √©poques) dans une session, Phase 2 (affinage, 100 √©poques) sur plusieurs sessions |
| **Sauvegarder les sorties** | Cliquer **"Save Version"** ‚Üí **"Save & Run All"** pour conserver les poids `best.pt` |
| **Utiliser les Datasets Kaggle** | T√©l√©charger votre dataset comme Dataset Kaggle pour un acc√®s instantan√© `/kaggle/input/` |
| **Surveiller l'utilisation GPU** | Utiliser `!nvidia-smi` p√©riodiquement pour v√©rifier l'utilisation VRAM |

#### Reprise d'Entra√Ænement Entre Sessions

```python
from ultralytics import YOLO

# Si la session a expir√© en cours d'entra√Ænement, reprendre depuis le dernier checkpoint :
# 1. T√©l√©charger last.pt depuis la sortie de la session pr√©c√©dente
# 2. Le t√©l√©verser comme Dataset Kaggle ou l'ajouter aux fichiers du notebook
# 3. Reprendre :

model = YOLO("/kaggle/input/previous-run/last.pt")  # Charger le checkpoint
results = model.train(resume=True)                    # Continue depuis l'arr√™t
```

### 9.4 R√©glage des Hyperparam√®tres

```python
# R√©glage automatique des hyperparam√®tres avec Ray Tune (sur Kaggle)
# Note : Ceci est gourmand en ressources ; r√©duire les it√©rations sur le tier gratuit
model = YOLO("yolov8s.pt")
result_grid = model.tune(
    data=f"{DATASET_PATH}/data.yaml",
    epochs=30,
    iterations=20,         # R√©duit pour les limites de temps Kaggle
    optimizer="AdamW",
    plots=True,
    save=True,
    val=True,
)
```

---

## 10. D√©ploiement & Int√©gration Embarqu√©e

### 10.1 Architecture de D√©ploiement

```mermaid
graph TB
    subgraph KAGGLE["üìì Entra√Ænement Kaggle"]
        NOTEBOOK["Notebook Kaggle<br/>(NVIDIA T4 x2 / P100)"]
        KAGGLE_DS["Dataset Kaggle<br/>(/kaggle/input/)"]
        TRAIN["Entra√Æner YOLOv8s<br/>(Transfert + Affinage)"]
        DOWNLOAD["T√©l√©charger best.pt<br/>(Save Version ‚Üí Output)"]
    end

    subgraph OPTIMIZATION["‚öôÔ∏è Optimisation"]
        EXPORT["Exporter le Mod√®le<br/>(.pt ‚Üí .onnx ‚Üí .engine)"]
        ONNX["ONNX Runtime<br/>(Multi-plateforme)"]
        TRT["TensorRT FP16<br/>(GPUs NVIDIA)"]
        OPENVINO["OpenVINO<br/>(CPUs Intel)"]
    end

    subgraph EDGE["üöó D√©ploiement Embarqu√©"]
        JETSON["NVIDIA Jetson Orin<br/>(40 TOPS)"]
        CAMERA["Module Cam√©ra<br/>(CSI / USB)"]
        CAN["Interface Bus CAN"]
        HUD["Affichage HUD"]
    end

    KAGGLE_DS --> NOTEBOOK
    NOTEBOOK --> TRAIN
    TRAIN --> DOWNLOAD
    DOWNLOAD --> EXPORT
    EXPORT --> ONNX
    EXPORT --> TRT
    EXPORT --> OPENVINO
    TRT --> JETSON
    CAMERA --> JETSON
    JETSON --> CAN
    JETSON --> HUD

    style KAGGLE fill:#20beff20,stroke:#20beff,color:#fff
    style OPTIMIZATION fill:#16213e,stroke:#0f3460,color:#fff
    style EDGE fill:#0f3460,stroke:#00d4ff,color:#fff
```

### 10.2 Commandes d'Export du Mod√®le

```python
from ultralytics import YOLO

# Charger le meilleur mod√®le (t√©l√©charg√© depuis la sortie Kaggle)
model = YOLO("best.pt")  # T√©l√©charg√© depuis la sortie du notebook Kaggle

# Exporter en ONNX (peut √™tre fait sur Kaggle ou localement)
model.export(format="onnx", imgsz=640, half=True, simplify=True)

# Exporter en TensorRT (pour NVIDIA Jetson ‚Äî faire ceci sur l'appareil Jetson)
model.export(format="engine", imgsz=640, half=True, device=0)

# Exporter en OpenVINO (pour Intel)
model.export(format="openvino", imgsz=640, half=True)
```

### 10.3 Sp√©cifications du Mat√©riel Embarqu√©

| Caract√©ristique | Jetson Orin Nano | Jetson Orin NX | Jetson AGX Orin |
|-----------------|------------------|----------------|-----------------|
| **Performance IA** | 40 TOPS | 100 TOPS | 275 TOPS |
| **GPU** | 1024 c≈ìurs Ampere | 1024 c≈ìurs Ampere | 2048 c≈ìurs Ampere |
| **CPU** | 6 c≈ìurs Cortex-A78 | 8 c≈ìurs Cortex-A78 | 12 c≈ìurs Cortex-A78 |
| **RAM** | 8 Go | 16 Go | 64 Go |
| **YOLOv8s FPS** | ~35 FPS | ~60 FPS | ~90+ FPS |
| **Prix (Est.)** | 199 $ | 399 $ | 999 $ |
| **Recommand√©** | ‚úÖ Budget | ‚úÖ **Meilleur Rapport Qualit√©/Prix** | Premium |

---

## 11. Phases de D√©veloppement & Calendrier

### 11.1 Aper√ßu des Phases (Diagramme de Gantt)

```mermaid
gantt
    title Syst√®me de D√©tection d'Objets par IA ‚Äî Calendrier de D√©veloppement
    dateFormat YYYY-MM-DD
    axisFormat %b %d

    section Phase 1: Recherche & Planification
    Revue de litt√©rature        :done, p1a, 2026-02-22, 7d
    Analyse des exigences       :done, p1b, 2026-02-22, 5d
    Conception architecture     :active, p1c, after p1a, 5d
    S√©lection stack technique   :p1d, after p1b, 3d

    section Phase 2: Cr√©ation du Dataset
    T√©l√©charger datasets publics :p2a, after p1c, 5d
    Enregistrement donn√©es perso :p2b, after p2a, 10d
    Annotation des donn√©es (CVAT):p2c, after p2b, 14d
    Conversion format YOLO      :p2d, after p2c, 3d
    Revue qualit√© et nettoyage  :p2e, after p2d, 5d
    Pipeline d'augmentation     :p2f, after p2e, 3d

    section Phase 3: D√©veloppement du Mod√®le
    Configuration environnement :p3a, after p2d, 2d
    Apprentissage transfert     :p3b, after p3a, 5d
    Affinage (Phase 2)          :p3c, after p3b, 10d
    R√©glage hyperparam√®tres     :p3d, after p3c, 5d
    Module estimation distance  :p3e, after p3b, 10d

    section Phase 4: Int√©gration
    Module prise de d√©cision    :p4a, after p3c, 7d
    Pipeline temps r√©el         :p4b, after p3e, 7d
    Multi-threading             :p4c, after p4b, 5d
    Int√©gration syst√®me         :p4d, after p4a, 5d

    section Phase 5: Tests & Optimisation
    √âvaluation du mod√®le        :p5a, after p3d, 5d
    D√©ploiement embarqu√©        :p5b, after p4d, 7d
    Optimisation TensorRT       :p5c, after p5b, 5d
    Tests en conditions r√©elles :p5d, after p5c, 10d

    section Phase 6: Documentation
    Documentation technique     :p6a, after p5d, 5d
    Rapport final               :p6b, after p6a, 5d
    Pr√©sentation                :p6c, after p6b, 3d
```

### 11.2 D√©tail des Phases

#### üìå Phase 1 : Recherche & Planification (Semaines 1‚Äì2)

| T√¢che | Description | Livrable |
|-------|-------------|----------|
| 1.1 | Revue des articles YOLO et documentation Ultralytics | Document de revue de litt√©rature |
| 1.2 | √âtude des m√©thodes d'estimation de distance (monoculaire, st√©r√©o) | Matrice de comparaison |
| 1.3 | Analyse des datasets existants pour la conduite autonome | Rapport de s√©lection de datasets |
| 1.4 | Conception de l'architecture syst√®me (tous les modules) | Diagrammes d'architecture |
| 1.5 | D√©finition des classes d'objets et des exigences | Tableau de sp√©cification des classes |
| 1.6 | S√©lection du mat√©riel et du stack logiciel | Document du stack technologique |

#### üìå Phase 2 : Cr√©ation du Dataset (Semaines 3‚Äì7)

| T√¢che | Description | Livrable |
|-------|-------------|----------|
| 2.1 | T√©l√©charger et pr√©traiter le dataset KITTI | Sous-ensemble KITTI au format YOLO |
| 2.2 | T√©l√©charger et pr√©traiter le dataset BDD100K | Sous-ensemble BDD100K au format YOLO |
| 2.3 | Filtrer les classes COCO pertinentes | Sous-ensemble COCO au format YOLO |
| 2.4 | T√©l√©charger et pr√©traiter GTSRB | Labels de d√©tection de panneaux |
| 2.5 | Enregistrer des s√©quences de conduite personnalis√©es (20+ heures) | Enregistrements vid√©o bruts |
| 2.6 | Extraire des images des enregistrements (2 FPS) | ~144K images brutes |
| 2.7 | S√©lectionner et filtrer les meilleures images | ~10K images s√©lectionn√©es |
| 2.8 | Annoter avec CVAT (bo√Ætes englobantes) | Fichiers de labels YOLO |
| 2.9 | Fusionner tous les datasets + unifier le mappage de classes | `data.yaml` unifi√© |
| 2.10 | Division Train/Val/Test 70/20/10 | Dataset final (~50K images) |
| 2.11 | Appliquer le pipeline d'augmentation | Jeu d'entra√Ænement augment√© |

#### üìå Phase 3 : D√©veloppement du Mod√®le (Semaines 5‚Äì9)

| T√¢che | Description | Livrable |
|-------|-------------|----------|
| 3.1 | Configurer le Notebook Kaggle avec acc√©l√©rateur GPU + t√©l√©verser le dataset | Environnement Kaggle fonctionnel |
| 3.2 | Entra√Æner YOLOv8s avec backbone gel√© (10 √©poques) | Poids Phase 1 |
| 3.3 | Affiner le mod√®le complet (100 √©poques) | Meilleurs poids Phase 2 |
| 3.4 | Lancer le r√©glage d'hyperparam√®tres (Ray Tune) | Hyperparam√®tres optimaux |
| 3.5 | Impl√©menter le module de calibration cam√©ra | Outil de calibration |
| 3.6 | Impl√©menter l'estimateur de distance g√©om√©trique | Module distance v1 |
| 3.7 | Int√©grer la profondeur monoculaire MiDaS | Module distance v2 |
| 3.8 | Impl√©menter le filtre de Kalman pour le lissage de distance | Distances liss√©es |

#### üìå Phase 4 : Int√©gration (Semaines 8‚Äì11)

| T√¢che | Description | Livrable |
|-------|-------------|----------|
| 4.1 | Impl√©menter le classificateur de zones de risque | Module d'√©valuation des risques |
| 4.2 | Impl√©menter le calculateur TTC | Pr√©diction de collision |
| 4.3 | Impl√©menter le planificateur d'action (freiner/diriger/avancer) | Moteur de d√©cision |
| 4.4 | Construire le pipeline vid√©o temps r√©el | Inf√©rence en streaming |
| 4.5 | Ajouter le suivi Bot-SORT / ByteTrack | Suivi multi-objets |
| 4.6 | Impl√©menter le pipeline multi-thread | D√©bit optimis√© |
| 4.7 | Construire la superposition HUD (visualisation OpenCV) | Sortie visuelle |
| 4.8 | Tests d'int√©gration syst√®me | Prototype int√©gr√© |

#### üìå Phase 5 : Tests & Optimisation (Semaines 10‚Äì14)

| T√¢che | Description | Livrable |
|-------|-------------|----------|
| 5.1 | √âvaluer mAP, pr√©cision, rappel sur le jeu de test | Rapport de performance |
| 5.2 | Profiler la vitesse d'inf√©rence sur le mat√©riel cible | Benchmarks de latence |
| 5.3 | Exporter en ONNX et TensorRT | Fichiers de mod√®le optimis√©s |
| 5.4 | Benchmark sur appareil Jetson | Mesures FPS |
| 5.5 | Tester sur des vid√©os de conduite r√©elles | Analyse qualitative |
| 5.6 | Tests de stress (nuit, pluie, √©blouissement) | Rapport de cas limites |
| 5.7 | Am√©liorer it√©rativement (r√©entra√Æner sur les √©checs) | Mod√®le am√©lior√© |

#### üìå Phase 6 : Documentation & Pr√©sentation (Semaines 14‚Äì16)

| T√¢che | Description | Livrable |
|-------|-------------|----------|
| 6.1 | R√©diger la documentation technique | Documentation technique compl√®te |
| 6.2 | Cr√©er des tableaux comparatifs de performance | Rapport de benchmark |
| 6.3 | Pr√©parer le rapport final du projet (PFE) | Rapport de projet |
| 6.4 | Cr√©er les diapositives de pr√©sentation | Pr√©sentation de soutenance |
| 6.5 | Enregistrer une vid√©o de d√©monstration | Vid√©o de d√©monstration |

---

## 12. Analyse des Risques & Att√©nuation

### 12.1 Matrice des Risques

```mermaid
quadrantChart
    title Matrice d'√âvaluation des Risques
    x-axis Impact Faible --> Impact √âlev√©
    y-axis Probabilit√© Faible --> Probabilit√© √âlev√©e
    quadrant-1 Surveiller
    quadrant-2 Critique - Att√©nuer
    quadrant-3 Accepter
    quadrant-4 Planifier la R√©ponse
    "Mauvaise d√©tection nocturne": [0.75, 0.80]
    "Donn√©es entra√Ænement insuffisantes": [0.65, 0.60]
    "Limitations m√©moire GPU": [0.40, 0.50]
    "Appareil embarqu√© trop lent": [0.70, 0.45]
    "Erreurs d'annotation": [0.55, 0.70]
    "D√©gradation m√©t√©o": [0.80, 0.65]
    "D√©s√©quilibre des classes": [0.50, 0.75]
    "D√©rive estimation distance": [0.60, 0.55]
```

### 12.2 Strat√©gies d'Att√©nuation des Risques

| Risque | Impact | Probabilit√© | Att√©nuation |
|--------|--------|-------------|-------------|
| **Mauvaise d√©tection nocturne** | √âlev√© | √âlev√©e | Ajouter des donn√©es sp√©cifiques de nuit, envisager cam√©ra IR |
| **D√©gradation m√©t√©orologique** | √âlev√© | Moyenne | Augmenter avec pluie/brouillard, utiliser le simulateur CARLA |
| **Donn√©es insuffisantes** | √âlev√© | Moyenne | Utiliser l'apprentissage par transfert, exploiter les grands datasets publics |
| **D√©s√©quilibre des classes** | Moyen | √âlev√©e | Sur-√©chantillonner les classes rares, utiliser la focal loss, augmentation copier-coller |
| **Erreurs d'annotation** | Moyen | √âlev√©e | Revue multi-personnes, outils d'annotation semi-automatique |
| **Vitesse appareil embarqu√©** | √âlev√© | Moyenne | TensorRT FP16, r√©duire la taille d'entr√©e, √©laguer le mod√®le |
| **D√©rive d'estimation de distance** | Moyen | Moyenne | Filtrage de Kalman, fusion de capteurs, recalibration r√©guli√®re |
| **Limites m√©moire GPU** | Moyen | Moyenne | Accumulation de gradient, entra√Ænement en pr√©cision mixte (FP16) |

---

## 13. M√©triques d'√âvaluation

### 13.1 M√©triques de D√©tection d'Objets

| M√©trique | Formule | Cible |
|----------|---------|-------|
| **mAP@0.5** | Pr√©cision Moyenne √† IoU 0.5 | ‚â• 0.85 |
| **mAP@0.5:0.95** | mAP sur plusieurs seuils IoU | ‚â• 0.60 |
| **Pr√©cision** | VP / (VP + FP) | ‚â• 0.90 |
| **Rappel** | VP / (VP + FN) | ‚â• 0.85 |
| **Score F1** | 2 √ó (P √ó R) / (P + R) | ‚â• 0.87 |
| **FPS** | Images trait√©es par seconde | ‚â• 30 |
| **Latence** | Temps d'inf√©rence de bout en bout | ‚â§ 33ms |

### 13.2 M√©triques d'Estimation de Distance

| M√©trique | Description | Cible |
|----------|-------------|-------|
| **EAM** | Erreur Absolue Moyenne (m√®tres) | ‚â§ 2.0m |
| **RMSE** | Racine de l'Erreur Quadratique Moyenne | ‚â§ 3.0m |
| **Err. Relative** | |Pr√©dit - R√©el| / R√©el √ó 100 | ‚â§ 10% |
| **Œ¥ < 1.25** | % de pr√©dictions √† 1.25√ó de la v√©rit√© terrain | ‚â• 85% |

### 13.3 M√©triques au Niveau Syst√®me

| M√©trique | Description | Cible |
|----------|-------------|-------|
| **FPS de Bout en Bout** | D√©bit complet du pipeline | ‚â• 30 FPS |
| **Latence de D√©cision** | Temps de la d√©tection au signal d'action | ‚â§ 50ms |
| **Taux de Fausses Alertes** | Freinages d'urgence inutiles / heure | ‚â§ 1 |
| **Taux de Manque** | Objets critiques non d√©tect√©s | ‚â§ 2% |
| **Consommation** | Consommation √©lectrique de l'appareil | ‚â§ 30W |

---

## 14. Structure du R√©pertoire du Projet

```
PFE/
‚îú‚îÄ‚îÄ README.md                          # Aper√ßu du projet
‚îú‚îÄ‚îÄ requirements.txt                   # D√©pendances Python
‚îú‚îÄ‚îÄ data.yaml                          # Configuration du dataset
‚îÇ
‚îú‚îÄ‚îÄ dataset/                           # Donn√©es d'entra√Ænement
‚îÇ   ‚îú‚îÄ‚îÄ train/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ images/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ labels/
‚îÇ   ‚îú‚îÄ‚îÄ val/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ images/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ labels/
‚îÇ   ‚îî‚îÄ‚îÄ test/
‚îÇ       ‚îú‚îÄ‚îÄ images/
‚îÇ       ‚îî‚îÄ‚îÄ labels/
‚îÇ
‚îú‚îÄ‚îÄ scripts/                           # Scripts utilitaires
‚îÇ   ‚îú‚îÄ‚îÄ convert_kitti_to_yolo.py       # Convertisseur format KITTI
‚îÇ   ‚îú‚îÄ‚îÄ convert_bdd_to_yolo.py         # Convertisseur format BDD100K
‚îÇ   ‚îú‚îÄ‚îÄ convert_coco_to_yolo.py        # Convertisseur format COCO
‚îÇ   ‚îú‚îÄ‚îÄ augmentation_pipeline.py       # Augmentations personnalis√©es
‚îÇ   ‚îú‚îÄ‚îÄ camera_calibration.py          # Outil de calibration cam√©ra
‚îÇ   ‚îî‚îÄ‚îÄ visualize_annotations.py       # Visualisation des labels
‚îÇ
‚îú‚îÄ‚îÄ src/                               # Code source
‚îÇ   ‚îú‚îÄ‚îÄ detection/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ detector.py                # Wrapper de d√©tection YOLO
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tracker.py                 # Suivi d'objets (Bot-SORT)
‚îÇ   ‚îú‚îÄ‚îÄ distance/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ geometric_estimator.py     # Distance mod√®le st√©nop√©
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ midas_estimator.py         # Estimation profondeur MiDaS
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fusion.py                  # Module de fusion de distance
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ kalman_filter.py           # Lissage temporel
‚îÇ   ‚îú‚îÄ‚îÄ decision/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ risk_assessor.py           # Score de risque par zone
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ttc_calculator.py          # Temps avant collision
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ action_planner.py          # D√©cisions d'action de conduite
‚îÇ   ‚îú‚îÄ‚îÄ pipeline/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ realtime_pipeline.py       # Pipeline principal temps r√©el
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ video_capture.py           # Capture multi-thread
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ hud_overlay.py             # Rendu visuel HUD
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ config.py                  # Gestion de configuration
‚îÇ       ‚îú‚îÄ‚îÄ logger.py                  # Utilitaires de journalisation
‚îÇ       ‚îî‚îÄ‚îÄ visualization.py           # Visualisation de d√©bogage
‚îÇ
‚îú‚îÄ‚îÄ configs/                           # Fichiers de configuration
‚îÇ   ‚îú‚îÄ‚îÄ model_config.yaml              # Hyperparam√®tres du mod√®le
‚îÇ   ‚îú‚îÄ‚îÄ camera_params.yaml             # Param√®tres intrins√®ques
‚îÇ   ‚îî‚îÄ‚îÄ decision_thresholds.yaml       # Seuils des zones de risque
‚îÇ
‚îú‚îÄ‚îÄ runs/                              # Sorties d'entra√Ænement
‚îÇ   ‚îî‚îÄ‚îÄ train/
‚îÇ       ‚îî‚îÄ‚îÄ driving_detector_v1/
‚îÇ           ‚îú‚îÄ‚îÄ weights/
‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ best.pt
‚îÇ           ‚îÇ   ‚îî‚îÄ‚îÄ last.pt
‚îÇ           ‚îî‚îÄ‚îÄ results.csv
‚îÇ
‚îú‚îÄ‚îÄ exports/                           # Mod√®les export√©s
‚îÇ   ‚îú‚îÄ‚îÄ best.onnx
‚îÇ   ‚îú‚îÄ‚îÄ best.engine                    # TensorRT
‚îÇ   ‚îî‚îÄ‚îÄ best_openvino/                 # OpenVINO
‚îÇ
‚îú‚îÄ‚îÄ notebooks/                         # Notebooks Jupyter
‚îÇ   ‚îú‚îÄ‚îÄ 01_data_exploration.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 02_training_analysis.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ 03_distance_calibration.ipynb
‚îÇ
‚îú‚îÄ‚îÄ tests/                             # Tests unitaires
‚îÇ   ‚îú‚îÄ‚îÄ test_detector.py
‚îÇ   ‚îú‚îÄ‚îÄ test_distance.py
‚îÇ   ‚îî‚îÄ‚îÄ test_decision.py
‚îÇ
‚îî‚îÄ‚îÄ docs/                              # Documentation
    ‚îú‚îÄ‚îÄ architecture.md
    ‚îú‚îÄ‚îÄ dataset_guide.md
    ‚îî‚îÄ‚îÄ deployment_guide.md
```

---

## 15. Diagramme de Classes (Conception Logicielle)

```mermaid
classDiagram
    class ObjectDetector {
        -model: YOLO
        -device: str
        -conf_threshold: float
        -iou_threshold: float
        +__init__(model_path, device, conf, iou)
        +detect(frame) List~Detection~
        +warmup()
    }

    class Detection {
        +bbox: Tuple[int,int,int,int]
        +class_id: int
        +class_name: str
        +confidence: float
        +track_id: int
        +distance: float
    }

    class ObjectTracker {
        -tracker_type: str
        -max_age: int
        +__init__(tracker_type)
        +update(detections, frame) List~Detection~
        +get_velocity(track_id) Tuple[float,float]
    }

    class DistanceEstimator {
        <<abstract>>
        +estimate(detection, frame) float
    }

    class GeometricEstimator {
        -focal_length: float
        -known_heights: Dict
        +estimate(detection, frame) float
    }

    class MiDaSEstimator {
        -model: MiDaS
        -transform: Transform
        +estimate(detection, frame) float
        +get_depth_map(frame) ndarray
    }

    class DistanceFusion {
        -geometric: GeometricEstimator
        -midas: MiDaSEstimator
        -alpha: float
        -kalman: KalmanFilter
        +estimate(detection, frame) float
    }

    class RiskAssessor {
        -zones: Dict[str, Tuple[float,float]]
        +assess(detection) RiskLevel
        +compute_ttc(distance, velocity) float
    }

    class ActionPlanner {
        -risk_assessor: RiskAssessor
        +plan(detections) Action
        +get_priority_action(actions) Action
    }

    class Action {
        +type: ActionType
        +intensity: float
        +direction: str
        +priority: int
    }

    class RealTimePipeline {
        -detector: ObjectDetector
        -tracker: ObjectTracker
        -distance: DistanceFusion
        -planner: ActionPlanner
        -capture: VideoCapture
        +run()
        +process_frame(frame) PipelineResult
        +stop()
    }

    DistanceEstimator <|-- GeometricEstimator
    DistanceEstimator <|-- MiDaSEstimator
    DistanceFusion --> GeometricEstimator
    DistanceFusion --> MiDaSEstimator
    RealTimePipeline --> ObjectDetector
    RealTimePipeline --> ObjectTracker
    RealTimePipeline --> DistanceFusion
    RealTimePipeline --> ActionPlanner
    ActionPlanner --> RiskAssessor
    ObjectDetector --> Detection
    ActionPlanner --> Action
```

---

## 16. Diagramme de Cas d'Utilisation

```mermaid
graph TB
    subgraph SYSTEM["Syst√®me de D√©tection d'Objets par IA"]
        UC1["D√©tecter les Objets Routiers"]
        UC2["Suivre les Objets Entre les Images"]
        UC3["Estimer la Distance des Objets"]
        UC4["√âvaluer le Risque de Collision"]
        UC5["G√©n√©rer une Action de Conduite"]
        UC6["Afficher la Superposition HUD"]
        UC7["Journaliser les Donn√©es T√©l√©matiques"]
        UC8["Entra√Æner un Mod√®le Personnalis√©"]
        UC9["Calibrer la Cam√©ra"]
    end

    DRIVER["üßë Conducteur"]
    VEHICLE["üöó ECU V√©hicule"]
    ENGINEER["üë®‚Äçüíª Ing√©nieur ML"]
    CAMERA["üìπ Cam√©ra"]

    CAMERA --> UC1
    UC1 --> UC2
    UC1 --> UC3
    UC2 --> UC4
    UC3 --> UC4
    UC4 --> UC5
    UC5 --> VEHICLE
    UC5 --> UC6
    UC6 --> DRIVER
    UC5 --> UC7
    ENGINEER --> UC8
    ENGINEER --> UC9

    style SYSTEM fill:#1a1a2e,stroke:#e94560,color:#fff,stroke-width:2px
```

---

## 17. Diagramme de D√©ploiement

```mermaid
graph TB
    subgraph KAGGLE["üìì Kaggle (Plateforme d'Entra√Ænement)"]
        KAGGLE_NB["Notebook Kaggle<br/>NVIDIA T4 x2 / P100<br/>Python 3.10 + CUDA"]
        KAGGLE_DS["Dataset Kaggle<br/>/kaggle/input/<br/>~50K images"]
        KAGGLE_OUT["Sortie Notebook<br/>best.pt / last.pt<br/>(T√©l√©charger les Poids)"]
    end

    subgraph LOCAL["üíª Machine Locale"]
        EXPORT_LOCAL["Export en ONNX<br/>Optimisation du Mod√®le"]
    end

    subgraph EDGE["üöó Unit√© Embarqu√©e V√©hicule"]
        JETSON["NVIDIA Jetson Orin NX<br/>Runtime TensorRT<br/>JetPack 6.0"]
        CAM["Module Cam√©ra<br/>CSI / USB 3.0<br/>1080p @ 30 FPS"]
        CANBUS["Interface Bus CAN<br/>Contr√¥le V√©hicule"]
        DISPLAY["√âcran HUD 7 pouces<br/>Sortie HDMI"]
        POWER["Alimentation 12V DC<br/>Batterie V√©hicule"]
    end

    subgraph NETWORK["üåê Mises √† Jour OTA"]
        OTA["Serveur de Mise √† Jour<br/>(Push de nouveaux poids)"]
    end

    KAGGLE_DS --> KAGGLE_NB
    KAGGLE_NB --> KAGGLE_OUT
    KAGGLE_OUT --> |T√©l√©charger best.pt| EXPORT_LOCAL
    EXPORT_LOCAL --> |Exporter .engine| JETSON
    CAM --> JETSON
    JETSON --> CANBUS
    JETSON --> DISPLAY
    POWER --> JETSON
    OTA -.-> |Mise √† jour WiFi| JETSON

    style KAGGLE fill:#20beff20,stroke:#20beff,color:#fff
    style LOCAL fill:#003049,stroke:#669bbc,color:#fff
    style EDGE fill:#1b4332,stroke:#52b788,color:#fff
    style NETWORK fill:#3c096c,stroke:#c77dff,color:#fff
```

---

## 18. Diagramme d'Activit√© (Boucle Principale de D√©tection)

```mermaid
stateDiagram-v2
    [*] --> Initialiser
    Initialiser --> CaptureImage
    
    CaptureImage --> Pr√©traiter
    Pr√©traiter --> Ex√©cuterYOLO
    Ex√©cuterYOLO --> ObjetsD√©tect√©s
    
    ObjetsD√©tect√©s --> AucunObjet: Pas de d√©tections
    ObjetsD√©tect√©s --> SuivreObjets: Objets trouv√©s
    
    AucunObjet --> MainteniVitesse
    MainteniVitesse --> CaptureImage
    
    SuivreObjets --> EstimerDistance
    EstimerDistance --> √âvaluerRisque
    
    √âvaluerRisque --> ZoneCritique: Distance < 5m
    √âvaluerRisque --> ZoneDanger: Distance 5-15m
    √âvaluerRisque --> ZoneAvertissement: Distance 15-30m
    √âvaluerRisque --> ZoneS√ªre: Distance > 30m
    
    ZoneCritique --> FreinageUrgence
    ZoneDanger --> FreinageFort
    ZoneAvertissement --> Ralentir
    ZoneS√ªre --> MainteniVitesse
    
    FreinageUrgence --> Journaliser√âv√©nement
    FreinageFort --> V√©rifierChangementVoie
    Ralentir --> Journaliser√âv√©nement
    
    V√©rifierChangementVoie --> VoieDisponible: Voie s√ªre
    V√©rifierChangementVoie --> FreinageSeul: Pas de voie s√ªre
    
    VoieDisponible --> Ex√©cuterChangementVoie
    FreinageSeul --> Journaliser√âv√©nement
    Ex√©cuterChangementVoie --> Journaliser√âv√©nement
    
    Journaliser√âv√©nement --> Mettre√ÄJourHUD
    Mettre√ÄJourHUD --> CaptureImage
```

---

## 19. R√©sum√©

Ce rapport pr√©sente une conception compl√®te d'un **syst√®me de d√©tection d'objets aliment√© par l'IA** pour la conduite autonome utilisant **Ultralytics YOLO**. Le syst√®me couvre :

| Module | Technologie Cl√© |
|--------|----------------|
| **D√©tection d'Objets** | YOLOv8s avec dataset personnalis√© de 18 classes |
| **Suivi d'Objets** | Suivi multi-objets Bot-SORT / ByteTrack |
| **Estimation de Distance** | Hybride g√©om√©trique + profondeur monoculaire MiDaS |
| **Prise de D√©cision** | √âvaluation des risques par zone + calculateur TTC |
| **Pipeline Temps R√©el** | Traitement multi-thread @ 30+ FPS |
| **D√©ploiement** | TensorRT sur NVIDIA Jetson Orin |

> [!TIP]
> **Prochaines √âtapes :** Commencer par la Phase 1 (Recherche & Planification), puis passer √† la Phase 2 (Cr√©ation du Dataset) qui est la t√¢che la plus chronophage. Le dataset personnalis√© est la fondation ‚Äî investissez du temps dans des annotations de qualit√© pour les meilleurs r√©sultats.

---

*Rapport g√©n√©r√© pour le Projet PFE ‚Äî D√©tection d'Objets par IA pour la Conduite Autonome*  
*Framework : [Ultralytics YOLO](https://github.com/ultralytics/ultralytics) | Licence : AGPL-3.0*
